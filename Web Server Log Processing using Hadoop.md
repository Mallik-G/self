# Web Server Log Processing using Hadoop

In this hadoop project, you will be using a sample application log file from an application server to a demonstrated scaled-down server log processing pipeline.

## What will you learn

- The benefits of log-mining in certain industries


- A full log-mining application use-case


- Using Flume to ingest log data


- Using Spark to process data


- Integrating Kafka to complex event alert


- Using Impala for the low-latency query of processed log data.


- Coordinating the data processing pipeline with Oozie.



## Project Description

Storing, processing and mining data from web server logs has become mainstream for a lot of companies today. Industry giants have used this engineering and the accompany science of machine learning to extract information that has helped in ads targeting, improved search, application optimization and general improvement in application's user experience.
In this hadoop project, we will be using a sample application log file from an application server to demonstrated a scaled-down server log processing pipeline. From ingestion to insight usually require Hadoop-ecosystem tools like [Flume](https://www.dezyre.com/hadoop-course/flume), [Pig](https://www.dezyre.com/hadoop-course/pig), Spark, [Hive](https://www.dezyre.com/hadoop-course/hive)/Impala, Kafka, [Oozie ](https://www.dezyre.com/hadoop-course/oozie)and [HDFS ](https://www.dezyre.com/hadoop-course/hdfs)for storage and this is what we will be looking at but holistically and specifically at each stage of the pipeline.

**Prerequisite:**

1. It is expected that students have a fair knowledge of Big Data and Hadoop.
2. Installation of the Cloudera quickstart vm is super-essential to get the best from this class. Instruction on how to setup a scala SDK and runtime can be found from [here](https://youtu.be/SFJsuo2XISs).



# Curriculum For This Mini Project

-    What are log files and types of log files
-    Contents of a log file
-    Uses of log files 
-    Process log file using Flume 
-    Ingest log data using Flume 
-    Using Spark to process data 
-    Downloads and Installations 
-    DoS Attacks and log files 
-    Using Apache Kafka for complex event processing 
-    Using Oozie to coordinate tasks 
-    Log file use-case 
-    Clone github repository and summary overview
-    Lambda Architecture for Data Infrastructure 
-    Solution Architecture overview 
-    Implement Flume Agent 
-    Troubleshooting Flume 
-    Spark Scale Execution 
-    Accumulator and execute hive table 
-    Impala execution 
-    Coordination tasks using Oozie 
-    Hue Workflow 
-    Running Oozie on command line



Description: https://www.dezyre.com/big-data-hadoop-projects/processing-web-server-log

GitHubLink: https://github.com/okmich/log-file-processing   [Refer to Batch Section]



